{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's learning of refactored model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define weaknesses of the approach from the article:\n",
    "1. Using of ResNet50 as backbone (Some more powerfull models can be used)\n",
    "2. Assumption about uniform distribution of data(num of lesions)\n",
    "3. Loss function and lambda parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done with first weakness. We can change it with more strong model like SWIN Transformer (tiny) from Microsoft. That model performed better in this task compared to ResNet50. Now, transformers are performing well not only in NLP tasks, but also in CV. However, that model provide new approach(patching). Also, we can add some self-attention layers.\n",
    "\n",
    "Second, some new approach to multi-task learning can be applied. In article they use standard hard parametr sharing, but also soft parametr sharing can be used. \n",
    "\n",
    "Also, X. Sun et al. proposed, <a href=\"https://arxiv.org/abs/1911.12423\">“AdaShare: Learning What to Share for Efficient Deep Multi-Task Learning”</a> (2020). The primary goal of the researchers was to specify a single multi-layer architecture for MTL and train a policy that determines which layers to share by multiple tasks, which layers to use for specific tasks and which layers to skip for all tasks while ensuring the model provides highest performance at its most compact form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second weakness is not too obvious. First, we can, that distribtion of data(from EDA) is not uniform (it's assumption, but the likehood of the value being below average is higher than the likelihood of being higher). Some other distributions can be tried (Student's t-distribution, Exponential and etc), but also we can try find by use of EM algorithm (data distribution can be mixture of gaussian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last question is about parametr lambda in loss function. We can try to learn that parametrs. I tried one approach from <a href=\"https://towardsdatascience.com/multi-task-learning-with-pytorch-and-fastai-6d10dc7ce855\"> here </a> and it didn't worked well. The problem is that model starts to optimize one loss and give small weights to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we cat try to expand dataset by some simple transformations (for example by albumination). From data we know, that majority of data have low bliriness(contrast). We can change it and add new images to dataset. So, it will help us to improve stability of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import to_absolute_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(config_path=\"./conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_local_train': {'data_path': '../Classification/JPEGImages', 'data_file': '../Classification/NNEW_trainval_0.txt', 'batch_size': 32, 'num_workers': 4}, 'dataset_local_test': {'data_path': '../Classification/JPEGImages', 'data_file': '../Classification/NNEW_test_0.txt', 'batch_size': 32, 'num_workers': 4}, 'optimizer': {'LR': 0.001, 'type': 'sgd'}, 'backbone': {'name': 'swin_tiny'}, 'logger': {'name': 'SGD SWIN Densed distribution 0.001', 'project': 'OpenFace'}, 'trainer': {'max_epochs': 20, 'log_every_n_steps': 3}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LDL import factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "\n",
    "    cfg['dataset_local_train']['data_path'] = to_absolute_path(cfg['dataset_local_train']['data_path'])\n",
    "    cfg['dataset_local_train']['data_file'] = to_absolute_path(cfg['dataset_local_train']['data_file'])\n",
    "\n",
    "    cfg['dataset_local_test']['data_path'] = to_absolute_path(cfg['dataset_local_test']['data_path'])\n",
    "    cfg['dataset_local_test']['data_file'] = to_absolute_path(cfg['dataset_local_test']['data_file'])\n",
    "\n",
    "    trainer, model, train_loader, test_loader = factory.get_trainer(\n",
    "        cfg['backbone']['name'],\n",
    "        cfg['dataset_local_train'],\n",
    "        cfg['dataset_local_test'],\n",
    "        cfg['logger'],\n",
    "        cfg['trainer']\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "    return trainer, model, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\OpenFaceTest\\venv\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msolidcake98\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/solidcake98/OpenFace/runs/316qdqrp\" target=\"_blank\">SGD SWIN Densed distribution 0.001</a></strong> to <a href=\"https://wandb.ai/solidcake98/OpenFace\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                      | Params\n",
      "----------------------------------------------------------------\n",
      "0 | multitask_model   | MultitaskModel            | 27.6 M\n",
      "1 | loss              | MultiTaskLossWrapperThird | 0     \n",
      "2 | train_mae         | MeanAbsoluteError         | 0     \n",
      "3 | train_accuracy    | Accuracy                  | 0     \n",
      "4 | valid_mae         | MeanAbsoluteError         | 0     \n",
      "5 | valid_mse         | MeanSquaredError          | 0     \n",
      "6 | valid_accuracy    | Accuracy                  | 0     \n",
      "7 | valid_precision   | Precision                 | 0     \n",
      "8 | valid_specificity | Specificity               | 0     \n",
      "9 | valid_sensetivity | Recall                    | 0     \n",
      "----------------------------------------------------------------\n",
      "27.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.6 M    Total params\n",
      "110.290   Total estimated model params size (MB)\n",
      "d:\\Projects\\OpenFaceTest\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:623: UserWarning: Checkpoint directory D:\\Projects\\OpenFaceTest\\checkpoints\\swin_t exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\OpenFaceTest\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:453: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/47 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\OpenFaceTest\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 47/47 [01:08<00:00,  1.45s/it, loss=0.46, v_num=dqrp, train_loss_step=0.645, valid_loss=0.981, train_loss_epoch=0.422]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<pytorch_lightning.trainer.trainer.Trainer at 0x23ed6cc5400>,\n",
       " LDLModel(\n",
       "   (multitask_model): MultitaskModel(\n",
       "     (feature_extraction): SwinTransformer(\n",
       "       (patch_embed): PatchEmbed(\n",
       "         (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "         (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "       (layers): Sequential(\n",
       "         (0): BasicLayer(\n",
       "           dim=96, input_resolution=(56, 56), depth=2\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): Identity()\n",
       "               (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (downsample): PatchMerging(\n",
       "             input_resolution=(56, 56), dim=96\n",
       "             (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "             (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "           )\n",
       "         )\n",
       "         (1): BasicLayer(\n",
       "           dim=192, input_resolution=(28, 28), depth=2\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (downsample): PatchMerging(\n",
       "             input_resolution=(28, 28), dim=192\n",
       "             (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "             (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           )\n",
       "         )\n",
       "         (2): BasicLayer(\n",
       "           dim=384, input_resolution=(14, 14), depth=6\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (2): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (3): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (4): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (5): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (downsample): PatchMerging(\n",
       "             input_resolution=(14, 14), dim=384\n",
       "             (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "             (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "           )\n",
       "         )\n",
       "         (3): BasicLayer(\n",
       "           dim=768, input_resolution=(7, 7), depth=2\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                 (act): GELU()\n",
       "                 (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "       (head): Identity()\n",
       "     )\n",
       "     (softamx): Softmax(dim=-1)\n",
       "     (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       "     (counting): Linear(in_features=768, out_features=65, bias=True)\n",
       "   )\n",
       "   (loss): MultiTaskLossWrapperThird()\n",
       "   (train_mae): MeanAbsoluteError()\n",
       "   (train_accuracy): Accuracy()\n",
       "   (valid_mae): MeanAbsoluteError()\n",
       "   (valid_mse): MeanSquaredError()\n",
       "   (valid_accuracy): Accuracy()\n",
       "   (valid_precision): Precision()\n",
       "   (valid_specificity): Specificity()\n",
       "   (valid_sensetivity): Recall()\n",
       " ),\n",
       " <torch.utils.data.dataloader.DataLoader at 0x23edd805f70>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x23ed6cc5280>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch process of learning here https://wandb.ai/solidcake98/ImageAcneAnalysis/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I provide the picture of working of web app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./jupyter_img/TestService.png\" width=1200 height=800/>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a105a4da89a176d1f07730376f264f731648881d1aace75cc56f6745fe1f2e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
